# -*- coding: utf-8 -*-
"""DataPoisoningAndroid

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wXocxpWia22_8bHw5g_i6WyP3ZNBTm3l
"""

import os
import numpy as np
import pandas as pd
import timeit
import datetime
import json

from enum import Enum
from functools import partial
from nltk.tokenize.regexp import regexp_tokenize
from sklearn.model_selection import StratifiedShuffleSplit, cross_validate
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix

!pip install tensorflow-gpu==2.0.0-alpha0
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Input, concatenate
from tensorflow.keras.callbacks import LambdaCallback
from tensorflow.keras import Model

from google.colab import drive
drive.mount('/content/gdrive')

"""CONSTANTS"""

#File paths
PROJECT_PATH = r"/content/gdrive/My Drive/ML/APKAnalysis"
DATA_PATH = os.path.join(PROJECT_PATH, r"Data/subsets/")
GOOD_PATH = os.path.join(DATA_PATH, r"benign_badging_{}000h.txt")
BAD_PATH = os.path.join(DATA_PATH, r"mal_badging_{}000h.txt")
RESULT_PATH = os.path.join(PROJECT_PATH, "Results")
#Data params
RANDOM_SEED = 42
SUBSET_COUNT = 4
TRAIN_RATIO = .8
class EMixStrategy(Enum):
  Random = 1
  Most_Occuring=2
#Model params
NEURONS_PER_LAYER = 32
DROPOUT_RATE = .1
INPUT_RATIO = .125 #How large should layers in the second network in the dual newtork be compared to the first 
BATCH_SIZE=128
EPOCHS=32

if not os.path.exists(RESULT_PATH):
  os.makedirs(RESULT_PATH)

ben_samples = []
mal_samples = []

for x in range(SUBSET_COUNT):
  with open(GOOD_PATH.format(x+1), encoding='utf-8') as f:
    ben_samples += f.readlines()
  with open(BAD_PATH.format(x+1), encoding='utf-8') as f:
    mal_samples += f.readlines()
  
samples = ben_samples + mal_samples
len(samples)

labels = np.array([])
for x in ben_samples:
  labels = np.append(labels, 0)
for x in mal_samples:
  labels = np.append(labels, 1)

sampleSize = len(labels)
sampleSize

"""Data Preprocessing"""

perm_pattern = "(?:\w|\.)+(?:permission).(?:\w|\.)+"
feat_pattern = "(?:\w|\.)+(?:hardware).(?:\w|\.)+"
comb_pattern = "(?:\w|\.)+(?:hardware|permission).(?:\w|\.)+"

"""Generate a matrix where the columns are the permissions and features, the rows are the instances, and the values are 1 if the permission/feature is present or not"""

perm_vect = CountVectorizer(analyzer=partial(regexp_tokenize, pattern=perm_pattern))
feat_vect = CountVectorizer(analyzer=partial(regexp_tokenize, pattern=feat_pattern))
comb_vect = CountVectorizer(analyzer=partial(regexp_tokenize, pattern=comb_pattern))

#If we didn't preload the vocabulary we'd call this
#perm_input_sparse = perm_vect.fit_transform(samples)
#feat_input_sparse = feat_vect.fit_transform(samples)
#comb_input_sparse = comb_vect.fit_transform(samples)

"""Preload the generated vocab over the full dataset so we don't have to call fit on the CountVectorizer"""

perm_vocab = json.load(open(os.path.join(PROJECT_PATH,'perm_vocab.json')))
feat_vocab = json.load(open(os.path.join(PROJECT_PATH,'feat_vocab.json')))
comb_vocab = json.load(open(os.path.join(PROJECT_PATH,'comb_vocab.json')))

perm_vect.vocabulary_ = perm_vocab
feat_vect.vocabulary_ = feat_vocab
comb_vect.vocabulary_ = comb_vocab

perm_input_sparse = perm_vect.transform(samples)
feat_input_sparse = feat_vect.transform(samples)
comb_input_sparse = comb_vect.transform(samples)

perm_inputs = perm_input_sparse.toarray()
feat_inputs = feat_input_sparse.toarray()
comb_inputs = comb_input_sparse.toarray()

print(feat_inputs.shape, "\n", feat_inputs) #Shows us for each instances what features it has

perm_vocab_width = len(perm_inputs[0])
feat_vocab_width = len(feat_inputs[0])
comb_vocab_width = len(comb_inputs[0])

perm_vocab_width, feat_vocab_width, comb_vocab_width

sss = StratifiedShuffleSplit(n_splits=1, random_state=0, test_size=1-TRAIN_RATIO)
for train_index, test_index in sss.split(perm_inputs, labels):
  perm_train, perm_test = perm_inputs[train_index], perm_inputs[test_index]
  feat_train, feat_test = feat_inputs[train_index], feat_inputs[test_index]
  comb_train, comb_test = comb_inputs[train_index], comb_inputs[test_index]
  labels_train, labels_test = labels[train_index], labels[test_index]

len(perm_train), "\n", len(feat_train)

labels_train

"""Model Creation"""

def createModel():
  perm_input_layer = Input(shape=(perm_vocab_width,), name='permissions_input')
  h1_perm = Dense(NEURONS_PER_LAYER, activation='relu')(perm_input_layer)
  h1_perm = Dropout(DROPOUT_RATE)(h1_perm)
  h2_perm = Dense(NEURONS_PER_LAYER, activation='relu')(h1_perm)
  feat_input_layer = Input(shape=(feat_vocab_width,), name='features_input')
  h1_feat = Dense(int(NEURONS_PER_LAYER*INPUT_RATIO), activation='relu')(feat_input_layer)
  #Combine last layer of first network (handling permissions)with first layer of the second network (handling features)
  h1_comb = concatenate([h2_perm, h1_feat])
  h2_comb = Dense(int((NEURONS_PER_LAYER+(NEURONS_PER_LAYER*INPUT_RATIO))/2), activation='relu')(h1_comb)
  h2_comb = Dropout(DROPOUT_RATE)(h2_comb)
  h3_comb = Dense(int((NEURONS_PER_LAYER+(NEURONS_PER_LAYER*INPUT_RATIO))/2), activation='relu')(h2_comb)
  output = Dense(1, activation='sigmoid', name="output")(h3_comb)
  model = Model(inputs=[perm_input_layer, feat_input_layer], outputs=output)
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  return model

"""Helper Functions"""

def calc_accuracy(cm):
    TP = float(cm[1][1])
    TN = float(cm[0][0])
    n_samples = cm.sum()
    return (TP+TN)/n_samples

def calc_precision(cm):
    TP = float(cm[1][1])
    FP = float(cm[0][1])
    return TP/(TP+FP)

def calc_recall(cm):
    TP = float(cm[1][1])
    FN = float(cm[1][0])
    return TP/(FN+TP)

def calc_f1(precision, recall):
    return 2*((precision*recall)/(precision+recall))
  
def save_results(data,mixStrategy):
  d = datetime.datetime.today()
  month = str( '%02d' % d.month)
  day = str('%02d' % d.day)
  hour = str('%02d' % d.hour)
  year = str('%02d' % d.year)
  min = str('%02d' % d.minute)

  df = pd.DataFrame(data)

  modelLabel = "mixP" + str(data[0]["label_mix_per"]) + "-size" + str(sampleSize) + "-strat" +  str(mixStrategy) + ".csv"
  modelPath = os.path.join(RESULT_PATH, modelLabel)
  print("Saving Results at " + modelPath + "!")
  
  isNewFile = not os.path.exists(modelPath)
  with open(modelPath, "a+") as f:
      df.to_csv(f, mode='a', header=isNewFile, index=False)

def mixLabels(y_train, perc, seed):
    '''
    Pick random labels and switch their values
    '''
    np.random.seed()
    mixSize = int(len(y_train) * perc)
    #Ensure fair chance to pick any label by mixing all of the possible indices, but also ensure no duplicates
    mixIndices = np.arange(y_train.shape[0]) 
    print("Label vector length and number of labels changed: ", y_train.shape[0], mixSize)
    np.random.shuffle(mixIndices)
    for i in np.arange(mixSize):
        y_train[mixIndices[i]] = 0 if y_train[mixIndices[i]] else 1
        
def importantVocabCount(row, X_cutoff):
    sum = 0
    for i in X_cutoff:
        sum += row[i]
    return sum
  
def mixLabelsByFeat(y_train, perc, seed, vocab_arr=comb_train):
    '''
    Mixes labels based on vocabulary counts
    '''
    #Convert sparse vocab count matrix to array, and find most frequently occuring feats/perms
    X_sum = np.sum(vocab_arr, axis=0)
    X_sum_sorted = np.argsort(X_sum)

    #50th most occuring word
    cutoff = X_sum_sorted[len(X_sum)-49:len(X_sum)-50:-1] 

    #Array with each instances word count for most frequent feats/perms
    X_wc = np.apply_along_axis(importantVocabCount, axis=1, arr=vocab_arr, X_cutoff=cutoff)
    #Array with indices of the instances with the highest word count for the most frequent feats/perms
    X_wc_i = np.argsort(X_wc.ravel())

    mixSize = int(len(y_train) * perc)
    mixIndices = X_wc_i[:-(mixSize+1): -1]
    
    print("Labelsize and number of labels changed: ", y_train.shape[0], mixSize)
    print("Indices of labels changed: ", mixIndices)
    print(len(y_train))
    
    for i in np.arange(mixSize):
        y_train[mixIndices[i]] = 0 if y_train[mixIndices[i]] else 1

def Test(mixP=0.0, mixStrategy=EMixStrategy.Random):
  labels_train_copy = labels_train.copy()
  
  if mixStrategy is EMixStrategy.Random:
    mixLabels(labels_train_copy, mixP, RANDOM_SEED)
  elif mixStrategy is EMixStrategy.Most_Occuring:
    mixLabelsByFeat(labels_train_copy, mixP, RANDOM_SEED)
  
  startTrain = timeit.default_timer()
  model.fit([perm_train, feat_train], labels_train_copy, epochs=EPOCHS, batch_size=BATCH_SIZE)
  endTrain = timeit.default_timer()
  labels_pred = model.predict([perm_test, feat_test], batch_size=BATCH_SIZE)
  endTest = timeit.default_timer()

  trainTime = endTrain - startTrain
  testTime = endTest - endTrain
  labels_pred = (labels_pred > 0.5)
  cm = confusion_matrix(labels_test, labels_pred)
  return (cm, trainTime, testTime, mixP, mixStrategy)
  
def calcMetrics(resTuple):
  cm = resTuple[0]
  acc = calc_accuracy(cm)
  prec = calc_precision(cm)
  rec = calc_recall(cm)
  f1 = calc_f1(prec, rec)
  print("Accuracy", acc, " Precision " ,prec, " Recall ", rec, " F1 ", f1)
  
  data = []
  data.append(dict(zip(["model_name", "neurons", "train_ratio", "input_ratio",
                        "epochs", "batch_size", "accuracy", "precision", "recall", "f1_score",
                        "train_time", "test_time", "label_mix_per", "label_mix_count"],
                        ["Dual_Large", NEURONS_PER_LAYER, TRAIN_RATIO, INPUT_RATIO, EPOCHS, BATCH_SIZE, acc, prec, rec, f1, resTuple[1], resTuple[2], resTuple[3], resTuple[3] * sampleSize])))
  save_results(data, resTuple[4])

mixPercentages = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
model = createModel() 
initial_weights = model.get_weights() 

for x in mixPercentages: 
  resTuple = Test(x, EMixStrategy.Most_Occuring) 
  calcMetrics(resTuple) 
  model.set_weights(initial_weights)

"""Finding optimal features based on gradients on trained model"""

model = createModel() 
print_weights = LambdaCallback(on_batch_end= lambda batch, logs: print(model.lay))
model.fit([perm_train, feat_train], labels_train_copy, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=)
calcMetrics(resTuple)

model.summary()

layer = model.get_layer('permissions_input')
len(layer.weights) #returns two lists containing weights and bias

layer.trainable_weights

